{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-analysis.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "8xywty-I-ztb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## load all the given datasets\n",
        "from string import punctuation\n",
        "\n",
        "with open(\"./hwk3_datasets/yelp-train.txt\",\"r\") as file:\n",
        "    ye_train = file.readlines()\n",
        "with open(\"./hwk3_datasets/yelp-valid.txt\",\"r\") as file:\n",
        "    ye_valid = file.readlines()\n",
        "with open(\"./hwk3_datasets/yelp-test.txt\",\"r\") as file:\n",
        "    ye_test = file.readlines()\n",
        "with open(\"./hwk3_datasets/IMDB-train.txt\",\"r\") as file:  ## here we will focus on the IMDB dataset. It is similar for yelp dataset\n",
        "    im_train = file.readlines()\n",
        "with open(\"./hwk3_datasets/IMDB-valid.txt\",\"r\") as file:\n",
        "    im_valid = file.readlines()\n",
        "with open(\"./hwk3_datasets/IMDB-test.txt\",\"r\") as file:\n",
        "    im_test = file.readlines()\n",
        "\n",
        "## lists to store the features and target variables\n",
        "im_train_f = []\n",
        "im_valid_f = []\n",
        "im_test_f = []\n",
        "im_train_t = []\n",
        "im_valid_t = []\n",
        "im_test_t = []\n",
        "\n",
        "for i in im_train:                   \n",
        "    s = i.rsplit(None,1)[0].lower()     ## convert the string to lowercase\n",
        "    im_train_f.append(''.join([l for l in s if l not in punctuation]))  ## append in the feature list, without punctuation\n",
        "    im_train_t.append(int(i.rsplit(None,1)[1]))      ## append in the target var list, in integer form\n",
        "for i in im_valid:\n",
        "    s = i.rsplit(None,1)[0].lower()\n",
        "    im_valid_f.append(''.join([l for l in s if l not in punctuation]))\n",
        "    im_valid_t.append(int(i.rsplit(None,1)[1]))\n",
        "for i in im_test:\n",
        "    s = i.rsplit(None,1)[0].lower()\n",
        "    im_test_f.append(''.join([l for l in s if l not in punctuation]))\n",
        "    im_test_t.append(int(i.rsplit(None,1)[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dG3n6XC6-ztg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## function that return a list of words in descending frequency, each tuple = (word, # of occurrence)\n",
        "def top_n_words(corpus,n):\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key = lambda x:x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "## the two feature sets for yelp and IMDB\n",
        "im_feature = top_n_words(im_train_f,10000)   ## top 10,000 words in IMDB, with frequency in each tuple\n",
        "\n",
        "vocab_im = []\n",
        "for t in im_feature:\n",
        "    vocab_im.append(t[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_T933d5-ztk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## the vec_XX_XXX_bin arrays are the desired binary representation\n",
        "vec_im = CountVectorizer()\n",
        "vec_im_bin = CountVectorizer(binary=True)\n",
        "Y = vec_im.fit_transform(vocab_im)\n",
        "Y_bin = vec_im_bin.fit_transform(vocab_im)\n",
        "vec_im_train = vec_im.transform(im_train_f).toarray()\n",
        "vec_im_train_bin = vec_im_bin.transform(im_train_f).toarray()\n",
        "vec_im_valid = vec_im.transform(im_valid_f).toarray()\n",
        "vec_im_valid_bin = vec_im_bin.transform(im_valid_f).toarray()\n",
        "vec_im_test = vec_im.transform(im_test_f).toarray()\n",
        "vec_im_test_bin = vec_im_bin.transform(im_test_f).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qKBiXZyRCg7d",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## function to calculate frequency bag of words\n",
        "def freq(arr):\n",
        "    array = []\n",
        "    for i in arr:\n",
        "        s = i.sum()\n",
        "        if s == 0:\n",
        "            array.append(np.zeros(arr.shape[1]))\n",
        "        else:\n",
        "            array.append(i.astype(float)/s)\n",
        "    return np.asarray(array)\n",
        "\n",
        "## now creaet the Frequency BoW representation, as np arrays.\n",
        "freq_im_train = freq(vec_im_train)\n",
        "freq_im_valid = freq(vec_im_valid)\n",
        "freq_im_test = freq(vec_im_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJkdk_xu-ztr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_im = vec_im.get_feature_names()\n",
        "id_im = np.arange(10000)\n",
        "fre_im = np.sum(vec_im_train,axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LyOGwTJO-ztt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "voc_im = np.c_[words_im,id_im,fre_im]\n",
        "\n",
        "## save the vocab.txt files\n",
        "np.savetxt('IMDB-vocab.txt',voc_im,fmt=\"%s\",delimiter='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O2w0XqWg-ztv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## function that converts a review to ids\n",
        "def conv(strr,voc):\n",
        "    arr = []\n",
        "    for s in strr.split():\n",
        "        if s in voc:\n",
        "            arr.append(voc.index(s))\n",
        "    str1 = ' '.join(str(e) for e in arr)\n",
        "    return str1\n",
        "\n",
        "## create the id encoded datapoints  - This chunk takes around 40 minutes.\n",
        "im_train_f_num = []\n",
        "for i in im_train_f:\n",
        "    im_train_f_num.append(conv(i,words_im))\n",
        "im_valid_f_num = []\n",
        "for i in im_valid_f:\n",
        "    im_valid_f_num.append(conv(i,words_im))\n",
        "im_test_f_num = []\n",
        "for i in im_test_f:\n",
        "    im_test_f_num.append(conv(i,words_im))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYP4kmFE-zt2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_im_train = np.c_[im_train_f_num,im_train_t]\n",
        "num_im_valid = np.c_[im_valid_f_num,im_valid_t]\n",
        "num_im_test = np.c_[im_test_f_num,im_test_t]\n",
        "## now save the required train/valid/test files.\n",
        "np.savetxt('IMDB-train.txt',num_im_train,fmt=\"%s\",delimiter='\\t')\n",
        "np.savetxt('IMDB-valid.txt',num_im_valid,fmt=\"%s\",delimiter='\\t')\n",
        "np.savetxt('IMDB-test.txt',num_im_test,fmt=\"%s\",delimiter='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fJ00vC57-zt6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vBhZPDil-zuu",
        "colab_type": "code",
        "colab": {},
        "outputId": "f96bb319-fb3a-4dc4-83a0-0d5a7b21c424"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Naive Bayes:\n",
        "\n",
        "## First do the hyperparameter tuning:\n",
        "param_grid3 = ParameterGrid({'alpha':[0.001,0.01,0.05,0.1,0.2,0.3,0.5,0.7,0.8,1]})\n",
        "scores3 = []\n",
        "for p in param_grid3:\n",
        "    clf = BernoulliNB(**p)\n",
        "    clf.fit(vec_im_train_bin,im_train_t)\n",
        "    pred = clf.predict(vec_im_valid_bin)\n",
        "    scores3.append(metrics.f1_score(im_valid_t,pred,average='micro'))\n",
        "print(scores3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8422999999999999, 0.8425, 0.843, 0.8431000000000001, 0.843, 0.8434, 0.8433, 0.8433, 0.843, 0.843]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UWZ08V5l-zuw",
        "colab_type": "code",
        "colab": {},
        "outputId": "2a384cf5-72a6-4f65-af4c-b1bed5bafb13"
      },
      "cell_type": "code",
      "source": [
        "## Then train the Naive Bayes model using best hyperparameter:\n",
        "nb3 = BernoulliNB(alpha = 0.3)\n",
        "nb3.fit(vec_im_train_bin,im_train_t)\n",
        "y_nb_im_train = nb3.predict(vec_im_train_bin)\n",
        "y_nb_im_valid = nb3.predict(vec_im_valid_bin)\n",
        "y_nb_im_test = nb3.predict(vec_im_test_bin)\n",
        "f1_nb_im_train = metrics.f1_score(im_train_t,y_nb_im_train,average='micro')\n",
        "f1_nb_im_valid = metrics.f1_score(im_valid_t,y_nb_im_valid,average='micro')\n",
        "f1_nb_im_test = metrics.f1_score(im_test_t,y_nb_im_test,average='micro')\n",
        "\n",
        "## f1-measure for train/valid/test for naive bayes:\n",
        "print(f1_nb_im_train)\n",
        "print(f1_nb_im_valid)\n",
        "print(f1_nb_im_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8727999999999999\n",
            "0.8434\n",
            "0.83552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "89s-eCWS-zuz",
        "colab_type": "code",
        "colab": {},
        "outputId": "228e6ab1-d10b-431a-d282-489f8b8c83b1"
      },
      "cell_type": "code",
      "source": [
        "## Decision Trees:\n",
        "## First do the hyperparameter tuning:  - this chunk takes several hours\n",
        "max_depth3 = [int(x) for x in np.linspace(2,32,num=6)]\n",
        "min_samples_split3 = [0.1,0.3,0.5,0.7,0.9,1.0]\n",
        "min_samples_leaf3 = [0.1,0.3,0.5]\n",
        "max_features3 = [1,100,1000,2000,5000,10000]\n",
        "param_grid_dt3 = ParameterGrid({'max_depth':max_depth3, 'min_samples_split':min_samples_split3,'min_samples_leaf':min_samples_leaf3,'max_features':max_features3})\n",
        "scores_dt3 = []\n",
        "for p in param_grid_dt3:\n",
        "    clf = DecisionTreeClassifier(**p)\n",
        "    clf.fit(vec_im_train_bin,im_train_t)\n",
        "    pred = clf.predict(vec_im_valid_bin)\n",
        "    scores_dt3.append(metrics.f1_score(im_valid_t,pred,average='micro'))\n",
        "print(scores_dt3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5346, 0.5847, 0.5, 0.5635, 0.5, 0.529, 0.5, 0.5, 0.5, 0.534, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.611, 0.5482, 0.5728, 0.5721, 0.5497, 0.5441, 0.5551, 0.5273, 0.5635, 0.5721, 0.5635, 0.5426, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5544, 0.5912, 0.6081, 0.5847, 0.5847, 0.5407, 0.5465, 0.5635, 0.5441, 0.5441, 0.5635, 0.5349, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.611, 0.5847, 0.611, 0.5721, 0.5847, 0.611, 0.5635, 0.5721, 0.5635, 0.5551, 0.5485, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.611, 0.611, 0.611, 0.611, 0.611, 0.611, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5485, 0.5273, 0.5269, 0.5211, 0.5355, 0.5635, 0.5, 0.5, 0.5, 0.5251, 0.524, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5923, 0.569, 0.6144, 0.5502, 0.54, 0.5551, 0.5485, 0.5043, 0.5442, 0.5389, 0.5426, 0.5465, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6375, 0.6156, 0.6143, 0.611, 0.538, 0.5497, 0.5721, 0.5442, 0.5635, 0.5721, 0.5441, 0.5456, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6567, 0.6343, 0.6296, 0.6103, 0.5721, 0.611, 0.5551, 0.5721, 0.5485, 0.5721, 0.5551, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6499, 0.649, 0.649, 0.611, 0.611, 0.611, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5648, 0.5, 0.546, 0.5225, 0.5022, 0.5, 0.5, 0.5, 0.5263, 0.5, 0.5255, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5598, 0.572, 0.5921, 0.5677, 0.611, 0.5407, 0.5387, 0.5273, 0.5635, 0.5635, 0.5441, 0.5401, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6392, 0.647, 0.5852, 0.5469, 0.556, 0.5721, 0.5441, 0.5349, 0.5465, 0.5721, 0.5485, 0.5441, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6403, 0.6403, 0.6374, 0.6312, 0.5847, 0.611, 0.5721, 0.5551, 0.5635, 0.5721, 0.5721, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6499, 0.649, 0.649, 0.611, 0.611, 0.611, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5275, 0.5245, 0.5613, 0.5078, 0.5068, 0.5162, 0.5233, 0.5, 0.5465, 0.5089, 0.5, 0.4967, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.576, 0.6259, 0.5605, 0.5465, 0.5721, 0.5279, 0.5349, 0.5456, 0.5551, 0.5426, 0.5419, 0.5273, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6118, 0.6256, 0.6256, 0.611, 0.5721, 0.5847, 0.5721, 0.5635, 0.5721, 0.5419, 0.5415, 0.5635, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.649, 0.6312, 0.649, 0.611, 0.611, 0.611, 0.5721, 0.5635, 0.5635, 0.5721, 0.5551, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6499, 0.649, 0.649, 0.611, 0.611, 0.611, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5308, 0.5456, 0.5151, 0.5, 0.5101, 0.5497, 0.5, 0.5, 0.5143, 0.5721, 0.5, 0.5069, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5838, 0.6312, 0.631, 0.5771, 0.5471, 0.5721, 0.5465, 0.5551, 0.5172, 0.5093, 0.5441, 0.5349, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6297, 0.6341, 0.6312, 0.556, 0.5497, 0.5847, 0.5426, 0.5635, 0.5465, 0.5415, 0.5721, 0.5441, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6431, 0.6388, 0.649, 0.5853, 0.5847, 0.556, 0.5721, 0.5551, 0.5465, 0.5721, 0.5635, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6499, 0.649, 0.649, 0.611, 0.611, 0.611, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5285, 0.5472, 0.5, 0.5204, 0.5, 0.5048, 0.5, 0.5, 0.5, 0.5441, 0.5349, 0.534, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5821, 0.5866, 0.606, 0.574, 0.5551, 0.5721, 0.5255, 0.5273, 0.5456, 0.5419, 0.5485, 0.5128, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5855, 0.649, 0.5831, 0.6274, 0.5428, 0.5721, 0.5721, 0.5456, 0.5465, 0.5635, 0.5419, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.635, 0.5949, 0.6116, 0.611, 0.611, 0.611, 0.5721, 0.5635, 0.5721, 0.5635, 0.5721, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6499, 0.649, 0.649, 0.611, 0.611, 0.611, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5721, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ewmlsnfO-zu3",
        "colab_type": "code",
        "colab": {},
        "outputId": "f7357543-b63c-4101-b903-6fc82f22095a"
      },
      "cell_type": "code",
      "source": [
        "## print the best hyperparameters found\n",
        "print(param_grid_dt3[np.argmax(scores_dt3)])\n",
        "print(scores_dt3[np.argmax(scores_dt3)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'min_samples_split': 0.1, 'min_samples_leaf': 0.1, 'max_features': 5000, 'max_depth': 8}\n",
            "0.6567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1IbZ2A27-zu6",
        "colab_type": "code",
        "colab": {},
        "outputId": "9386b8c9-1920-40ff-9f26-51357beaed9e"
      },
      "cell_type": "code",
      "source": [
        "## Then train the DecisionTree model using the best hyperparameters:\n",
        "dt3 = DecisionTreeClassifier(max_depth=8, min_samples_split = 0.1, min_samples_leaf = 0.1, max_features=5000)\n",
        "dt3.fit(vec_im_train_bin,im_train_t)\n",
        "y_dt_im_train = dt3.predict(vec_im_train_bin)\n",
        "y_dt_im_valid = dt3.predict(vec_im_valid_bin)\n",
        "y_dt_im_test = dt3.predict(vec_im_test_bin)\n",
        "f1_dt_im_train = metrics.f1_score(im_train_t,y_dt_im_train,average='micro')\n",
        "f1_dt_im_valid = metrics.f1_score(im_valid_t,y_dt_im_valid,average='micro')\n",
        "f1_dt_im_test = metrics.f1_score(im_test_t,y_dt_im_test,average='micro')\n",
        "\n",
        "## f1-measure for train/valid/test for decision trees:\n",
        "print(f1_dt_im_train)\n",
        "print(f1_dt_im_valid)\n",
        "print(f1_dt_im_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.633\n",
            "0.6312\n",
            "0.62764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uukreiGE-zu8",
        "colab_type": "code",
        "colab": {},
        "outputId": "6a6199a5-48c2-405f-ef10-8af9798d6d50"
      },
      "cell_type": "code",
      "source": [
        "## Linear SVM:\n",
        "## First do the hyperparameter tuning:\n",
        "param_grid_svm3 = ParameterGrid({'C':[0.001,0.01,0.1,1,10,25,50,100,1000]})\n",
        "scores_svm3 = []\n",
        "for p in param_grid_svm3:\n",
        "    clf = LinearSVC(**p)\n",
        "    clf.fit(vec_im_train_bin,im_train_t)\n",
        "    pred = clf.predict(vec_im_valid_bin)\n",
        "    scores_svm3.append(metrics.f1_score(im_valid_t,pred,average='micro'))\n",
        "print(scores_svm3)  ## print f1-scores for different values of C"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8688, 0.8759, 0.8582, 0.8442, 0.8407, 0.8413000000000002, 0.8411, 0.8404, 0.8406]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L3JaJEZ0-zu_",
        "colab_type": "code",
        "colab": {},
        "outputId": "c15794f6-bc96-478a-f2e7-63afba099281"
      },
      "cell_type": "code",
      "source": [
        "## Then train the Linear SVM model using best hyperparameter:\n",
        "svm3 = LinearSVC(C=0.01)\n",
        "svm3.fit(vec_im_train_bin,im_train_t)\n",
        "y_svm_im_train = svm3.predict(vec_im_train_bin)\n",
        "y_svm_im_valid = svm3.predict(vec_im_valid_bin)\n",
        "y_svm_im_test = svm3.predict(vec_im_test_bin)\n",
        "f1_svm_im_train = metrics.f1_score(im_train_t,y_svm_im_train,average='micro')\n",
        "f1_svm_im_valid = metrics.f1_score(im_valid_t,y_svm_im_valid,average='micro')\n",
        "f1_svm_im_test = metrics.f1_score(im_test_t,y_svm_im_test,average='micro')\n",
        "\n",
        "## f1-measure for train/valid/test for linear SVM:\n",
        "print(f1_svm_im_train)\n",
        "print(f1_svm_im_valid)\n",
        "print(f1_svm_im_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9636666666666667\n",
            "0.8759\n",
            "0.87116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YvMnAw2q-zvL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}